model_name: distilbert-base-uncased
output_dir: ./results

# Data controls (smaller = faster for first run)
train_samples: 2000   # set null to use full train split
eval_samples: 1000    # set null to use full test split

# Training
num_train_epochs: 1
learning_rate: 2e-5
per_device_train_batch_size: 4
per_device_eval_batch_size: 8
weight_decay: 0.01
evaluation_strategy: epoch
seed: 42
